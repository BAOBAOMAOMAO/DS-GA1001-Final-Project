{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Code here\n",
    "# colnames = [\"age\",\"job\",\"marital\",\"education\", \"default\", \"balance\", \"housing\", \"loan\", \"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"]\n",
    "df = pd.read_csv(\"./bank-additional/bank-additional-full.csv\", sep=';')\n",
    "\n",
    "# Create a new data set with y transfored to binary data\n",
    "\n",
    "def transform(string):\n",
    "    if string==\"yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "y_trans = df['y'].apply(transform)\n",
    "new_df = df\n",
    "new_df['y'] = y_trans\n",
    "new_df.head(5)\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "Deal with unknown data\n",
    "Strategy:\n",
    "\n",
    "- For continuous data, change it with average \n",
    "- For categorical data, change it to a new category\n",
    "\n",
    "The result is there is no NaNs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age False\n",
      "job False\n",
      "marital False\n",
      "education False\n",
      "default False\n",
      "housing False\n",
      "loan False\n",
      "contact False\n",
      "month False\n",
      "day_of_week False\n",
      "duration False\n",
      "campaign False\n",
      "pdays False\n",
      "previous False\n",
      "poutcome False\n",
      "emp.var.rate False\n",
      "cons.price.idx False\n",
      "cons.conf.idx False\n",
      "euribor3m False\n",
      "nr.employed False\n",
      "y False\n"
     ]
    }
   ],
   "source": [
    "# Check number of NaNs\n",
    "for col in new_df.columns:\n",
    "    print(col + ' ' + str(new_df.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data -- Pdays\n",
    "\n",
    "pdays has 39673 of 999s --- this is missing data.\n",
    "\n",
    "It is a continuous variable, so replace 999 with mean, while removing 999 first.\n",
    "\n",
    "This data set also contains missing data on one numerical feature: pdays. This feature indicates the number of days that passed by after the client was last contacted from a previous campaign- it was coded as '999' if the contact never happened. Over 90% of the records show a number missing for the pdays. In order to implement the machine learning algorthms, we need to imput the missing values of this feature in a way to maximize its prediction accuracy.\n",
    "\n",
    "In the study, we tested the following appraches with the Logistic Regression algorithm:\n",
    "\n",
    "- Leave it as it is (999)\n",
    "- Imput as the column mean\n",
    "- Imput as zero\n",
    "- Remove the column from data set\n",
    "\n",
    "Since the result shows the highest prediction accuracy on the first apprach. we will keep the data as it is in the rest of study.\n",
    "\n",
    "Reference: https://nycdatascience.com/blog/student-works/machine-learning/machine-learning-retail-bank-marketing-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Imput as the column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df2 = new_df.copy() # this creates a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df2['pdays'].replace(999, (new_df2['pdays'] != 999).mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method3: Imput as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df3 = new_df.copy() # this creates a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df3['pdays'].replace(999, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df4 = new_df.drop('pdays', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "Now do the proprocessing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "job\n",
      "admin.           10422\n",
      "blue-collar       9254\n",
      "entrepreneur      1456\n",
      "housemaid         1060\n",
      "management        2924\n",
      "retired           1720\n",
      "self-employed     1421\n",
      "services          3969\n",
      "student            875\n",
      "technician        6743\n",
      "unemployed        1014\n",
      "unknown            330\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "marital\n",
      "divorced     4612\n",
      "married     24928\n",
      "single      11568\n",
      "unknown        80\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "education\n",
      "basic.4y                4176\n",
      "basic.6y                2292\n",
      "basic.9y                6045\n",
      "high.school             9515\n",
      "illiterate                18\n",
      "professional.course     5243\n",
      "university.degree      12168\n",
      "unknown                 1731\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "default\n",
      "no         32588\n",
      "unknown     8597\n",
      "yes            3\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "housing\n",
      "no         18622\n",
      "unknown      990\n",
      "yes        21576\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "loan\n",
      "no         33950\n",
      "unknown      990\n",
      "yes         6248\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "contact\n",
      "cellular     26144\n",
      "telephone    15044\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "month\n",
      "apr     2632\n",
      "aug     6178\n",
      "dec      182\n",
      "jul     7174\n",
      "jun     5318\n",
      "mar      546\n",
      "may    13769\n",
      "nov     4101\n",
      "oct      718\n",
      "sep      570\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "day_of_week\n",
      "fri    7827\n",
      "mon    8514\n",
      "thu    8623\n",
      "tue    8090\n",
      "wed    8134\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "poutcome\n",
      "failure         4252\n",
      "nonexistent    35563\n",
      "success         1373\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# examine all variables that are object, store them in a list\n",
    "obj_col_list = []\n",
    "for column in new_df.columns:\n",
    "    if new_df.dtypes[column] == 'object':\n",
    "        print('\\n')\n",
    "        print(new_df.groupby([column]).size())\n",
    "        obj_col_list.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. One-hot encoding of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# a function to do one-hot-encoding for categorical variables\n",
    "# input is an nparray of categorical values, ncol = 1, nrow = len(dataset)\n",
    "# output is a pd dataframe of one-hot vectors. ncol = number of categoreis, nrow = len(dataset)\n",
    "def one_hot_encoding(values):\n",
    "    # first do the one-hot encoding. return a np array of vectors.\n",
    "    ori_index = values.index\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values) #  \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    # then transform the vector into pd dataframe with proper column names\n",
    "    col_names = [values.name + '_' + x for x in label_encoder.classes_.tolist()]\n",
    "    one_hot_encoding_df = pd.DataFrame(onehot_encoded.tolist(), columns = col_names, index = ori_index)\n",
    "\n",
    "    return one_hot_encoding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def integer_encoding(values):\n",
    "    # first do the one-hot encoding. return a np array of vectors.\n",
    "    ori_index = values.index\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values) #  \n",
    "    return integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'poutcome']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part does the transformation of one-hot vectors\n",
    "num_col_list = list(set(new_df.columns).difference(obj_col_list)) # the list of columns that are of numerical values\n",
    "num_col_list\n",
    "new_df_num = new_df[num_col_list]\n",
    "new_df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in obj_col_list:\n",
    "    df_mini = one_hot_encoding(new_df[col])\n",
    "    new_df_num = pd.concat([new_df_num, df_mini], axis=1, join_axes=[new_df_num.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_features = new_df_num.drop(labels = 'y', axis = 1)\n",
    "data_target = new_df_num['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_target, test_target = train_test_split(data_features, data_target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28831, 63)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28831,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cross validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.KFold(n=41188, n_folds=5, shuffle=False, random_state=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kfolds = KFold(data_features.shape[0], n_folds = 5)\n",
    "\n",
    "kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "SVM_parameters = {'kernel':('sigmoid', 'rbf', 'poly'), 'C':[1, 10, 100]}\n",
    "lr_parameters = dict(C = [10**i for i in range(-3, 3)],\n",
    "                  penalty = ['l1', 'l2'])\n",
    "knn_parameters = {'n_neighbors': [5, 10, 15, 20], 'weights':['uniform', 'distance'], 'leaf_size':[20,30,40]}\n",
    "nn_parameters = dict(hidden_layer_sizes=[(30,),(50,),(70,),(100,)], activation=['relu', 'tanh', 'logistic'], learning_rate=['constant', 'adaptive'])\n",
    "decision_parameters = dict(max_depth=[5, 10, 15, 20], min_samples_split=[2,6,12], min_samples_leaf=[1,10,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906123385442\n"
     ]
    }
   ],
   "source": [
    "lr_grid_search = GridSearchCV(linear_model.LogisticRegression(), lr_parameters, cv = kfolds, scoring = 'roc_auc') \n",
    "lr_grid_search.fit(data_features, data_target)\n",
    "\n",
    "\n",
    "#3rd, get the score of the best model and print it\n",
    "best_1r_1 = lr_grid_search.best_score_\n",
    "print(best_1r_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best estimator\n",
    "lr_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the results of LR \n",
    "y_pred_lr = best_1r_1.pred(test_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_parameters, cv = kfolds, scoring = 'roc_auc') \n",
    "knn_grid_search.fit(data_features, data_target)\n",
    "\n",
    "#3rd, get the score of the best model and print it\n",
    "best_knn_1 = knn_grid_search.best_score_\n",
    "print(best_knn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(random_state=43)\n",
    "nn_grid_search = GridSearchCV(mlp, nn_parameters, cv = kfolds, scoring = 'roc_auc') \n",
    "nn_grid_search.fit(data_features, data_target)\n",
    "\n",
    "#3rd, get the score of the best model and print it\n",
    "best_nn_1 = nn_grid_search.best_score_\n",
    "print(best_nn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(), decision_parameters, cv = kfolds, scoring = 'roc_auc') \n",
    "dt_grid_search.fit(data_features, data_target)\n",
    "\n",
    "#3rd, get the score of the best model and print it\n",
    "best_dt_1 = dt_grid_search.best_score_\n",
    "print(best_dt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# defines a classification tree\n",
    "def NBClassifier(X_train,y_train,X_test,y_test, auc):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test),y_test)\n",
    "        return cm # (cm[0][0]+cm[1][1])/float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = NBClassifier(train_features,train_target,test_features,test_target, 1)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# defines a classification tree\n",
    "def NNClassifier(X_train,y_train,X_test,y_test, auc):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(30, ),learning_rate='adaptive', random_state=43)\n",
    "    clf.fit(X_train,y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test),y_test)\n",
    "        return cm # (cm[0][0]+cm[1][1])/float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = NNClassifier(train_features,train_target,test_features,test_target, 1)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# defines a classification tree\n",
    "def KNNClassifier(X_train,y_train,X_test,y_test, auc):\n",
    "    clf = KNeighborsClassifier(n_neighbors=15)\n",
    "    clf.fit(X_train,y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test),y_test)\n",
    "        return cm # (cm[0][0]+cm[1][1])/float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = KNNClassifier(train_features,train_target,test_features,test_target, 1)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# defines a classification tree\n",
    "def SVMClassifier(X_train,y_train,X_test,y_test, auc):\n",
    "    clf = svm.SVC(C=1.0, kernel='rbf', probability=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test),y_test)\n",
    "        return cm # (cm[0][0]+cm[1][1])/float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test,clf.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = SVMClassifier(train_features,train_target,test_features,test_target, 1)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(train_features, train_target)\n",
    "\n",
    "# make predictions for test data\n",
    "xgboost_pred = model.predict(test_features)\n",
    "predictions = [round(value) for value in xgboost_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test_target, xgboost_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression\n",
    "The binary classification goal is to predict if the client will subscribe a bank term deposit (variable y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_features, train_target)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "predict_y = regr.predict(test_features)\n",
    "\n",
    "\n",
    "# The predict score\n",
    "print(\"Mean accuracy Score: %.2f\" % regr.score(test_features,test_target))\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(test_target, predict_y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_target, predict_y))\n",
    "# The coefficients\n",
    "# print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target, predict_y)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1,label=\"ROC of lr\" +\"(AUC = %0.4f)\" %roc_auc)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve and AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_target, predict_y).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "##################################################\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(regr, new_df_num.drop(labels = 'y', axis = 1), new_df_num[\"y\"], cv=50)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# defines a classification tree\n",
    "def testTrees(X_train,y_train,X_test,y_test,dep,leaf,auc):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy',min_samples_leaf=leaf,max_depth=dep)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test),y_test)\n",
    "        return (cm[0][0]+cm[1][1])/float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lab='y_buy'\n",
    "\n",
    "depths=[4,5,10,20]\n",
    "leaves=np.arange(1,101)\n",
    "\n",
    "#Run all of the options\n",
    "run=1\n",
    "if (run==1):\n",
    "    #Initialize dictionary of results\n",
    "    res=dict()\n",
    "    for d in depths:\n",
    "        res[d]=list()\n",
    "\n",
    "    #Now train and get results for each option\n",
    "    for d in depths:\n",
    "        for l in leaves:\n",
    "            res[d].append(testTrees(train_features,train_target,test_features,test_target, d, l, 1))\n",
    "\n",
    "\n",
    "#Now plot            \n",
    "fig = plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "plt.plot(leaves,res[depths[0]],'b-',label='Depth={}'.format(depths[0]))\n",
    "plt.plot(leaves,res[depths[1]],'r-',label='Depth={}'.format(depths[1]))\n",
    "plt.plot(leaves,res[depths[2]],'y-',label='Depth={}'.format(depths[2]))\n",
    "plt.plot(leaves,res[depths[3]],'g-',label='Depth={}'.format(depths[3]))\n",
    "plt.legend(loc=4)\n",
    "ax.set_xlabel('Min Leaf Size')\n",
    "ax.set_ylabel('Test Set AUC')\n",
    "plt.title('Holdout AUC by Hyperparameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "# import course_utils as bd\n",
    "# imp.reload(bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll build a RF and compare to a DT\n",
    "clf_def = DecisionTreeClassifier(criterion='entropy', min_samples_leaf = 20)\n",
    "clf_def = clf_def.fit(train_features, train_target)\n",
    "# dt_pred = clf_def.predict_proba(test_features)\n",
    "dt_pred = clf_def.predict(test_features)\n",
    "\n",
    "rf_def = RandomForestClassifier(criterion='entropy', n_estimators=100)\n",
    "rf_def = rf_def.fit(train_features, train_target)\n",
    "# rf_pred = rf_def.predict_proba(test_features)\n",
    "rf_pred = rf_def.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to plot AUC, from course Git\n",
    "\n",
    "def plotAUC(truth, pred, lab):\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "    plt.plot(fpr, tpr, color=c, label= lab+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "plotAUC(test_target, dt_pred, 'DT')\n",
    "plotAUC(test_target, rf_pred, 'RF')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
